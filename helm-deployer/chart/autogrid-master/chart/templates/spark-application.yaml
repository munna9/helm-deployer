#
# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Support for Python is experimental, and requires building SNAPSHOT image of Apache Spark,
# with `imagePullPolicy` set to Always

apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: {{ .Values.metadata.fullName }}
  namespace: {{ .Values.metadata.namespace }}
spec:
  type: Python
  pythonVersion: "3"
  mode: cluster
  imagePullSecrets:
    - registry-secret
  image: {{ .Values.sparkApp.image }}
  imagePullPolicy: Always
  mainApplicationFile: {{ .Values.sparkApp.mainApplicationFile }}
  sparkVersion: "3.0.0-preview"
  restartPolicy:
    type: OnFailure
    onFailureRetries: 100
    onFailureRetryInterval: 10
    onSubmissionFailureRetries: 5
    onSubmissionFailureRetryInterval: 20
  volumes:
    - name: spark-app
      persistentVolumeClaim:
        claimName: {{ .Values.pvc.name }}
    # - name: spark-cm
    #     configMap:
    #       name: dummy-cm
  driver:
    cores: {{ .Values.spec.driver.cores }}
    coreLimit: {{ .Values.spec.driver.coreLimit }}
    memory: {{ .Values.spec.driver.memory }}
    labels:
      version: {{ .Values.spec.driver.labels.version }}
    serviceAccount: {{ .Values.spec.driver.serviceAccount }}
    secrets:
      - name: spark-secrets
        path: /etc/secrets
        secretType: Generic
    volumeMounts:
      - name: spark-app
        mountPath: /var/lib/sparkapp
      # - name: spark-cm
      #   mountPath: /opt/spark/mycm
  executor:
    cores: {{ .Values.spec.executor.cores }}
    instances: {{ .Values.spec.executor.instances }}
    memory: {{ .Values.spec.executor.memory }}
    labels:
      version: {{ .Values.spec.executor.labels.version }}
  sparkConf:
    "spark.hadoop.mapreduce.outputcommitter.factory.scheme.s3a": " org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory"
    "spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version": "2"
    "spark.hadoop.fs.s3a.committer.name": "partitioned"
    "spark.hadoop.fs.s3a.committer.magic.enabled": "false"
    "spark.hadoop.fs.s3a.commiter.staging.conflict-mode": "append"
    "spark.hadoop.fs.s3a.committer.staging.unique-filenames": "true"
    "spark.hadoop.fs.s3a.committer.staging.abort.pending.uploads": "true"
    "spark.hadoop.mapreduce.outputcommitter.factory.scheme.s3a": "org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory"
    # "spark.sql.sources.commitProtocolClass": "org.apache.spark.internal.io.cloud.PathOutputCommitProtocol"
    # "spark.sql.parquet.output.committer.class": "org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter"
  hadoopConf:
    "fs.s3.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem"
    "fs.s3a.aws.credentials.provider" : "org.apache.hadoop.fs.s3a.AssumedRoleCredentialProvider"
    "fs.s3a.aws.credentials.provider" : "org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider"
    "fs.s3a.multiobjectdelete.enable": "false"
    "fs.s3a.assumed.role.session.duration" : "30m"
    "fs.s3a.assumed.role.sts.endpoint": "sts.us-east-1.amazonaws.com"
    "fs.s3a.assumed.role.sts.endpoint.region": "us-east-1"
    "fs.s3a.assumed.role.credentials.provider": "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider"
